{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # SENTIMENT ANALYSIS \n",
    " \n",
    " ### TO PREDICT POSITIVE AND NEGATIVE SENTENCES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What is sentiment analysis ?\n",
    "\n",
    "The process of computationally identifying and categorizing opinions expressed in a piece of text whether the writer's attitude towards a particular topic, product, etc., is positive, negative, neutral … etc.\n",
    "\n",
    "My goal was to predict if a sentence entered by the user exhibits a  positive or negative sentiment.\n",
    "Initially targeted to all casual users who want to try this utility for fun. \n",
    "Later on my study on sentiment analysis would be applied to the youtube videos to predict if the video is positive or negative."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. <b>PRILIM STUDY</b>\n",
    "\n",
    "I started with a priliminary study to know the working of sentimen analysis. Where I used a small data set from the below link\n",
    "https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences.\n",
    "DATA : The data had a total of 10662 rows with 5331 Positive sentences and 5331 Negative sentences.\n",
    "CLEANING : I did a basic cleaning by removing all the html, non-letters and stop-words.\n",
    "\n",
    "\n",
    "<img src=\"pics/random_forest.png\" width=\"500px\">\n",
    "\n",
    "The above fgure is showing how I did sentiment analysis using random forest.\n",
    "Corpus : Is the collection of documents.\n",
    "\n",
    "- Clean the corpus : Here is where I did my data cleaning where I removed all the html, non-letters and stop-words.\n",
    "\n",
    "- Bag of Words : It is a numerical representation of a piece of text. Here we are counting the frequency of the                        occurence of a piece of text and we create a dictionary and this process is called \"Tokenization\".\n",
    "\n",
    "- Creating feature vectors : Using count vectorizer I converted the Bag of words created above to array format \n",
    "  Count Vectorizer counts the occurences of all the words in a piece of text.                         \n",
    "\n",
    "- Creating classifier - A classifier is basically a model that would help us in predicting the result. Here I used                             \"Random Forest\"\n",
    "\n",
    "The communication of random forest is as shown below:\n",
    "\n",
    "\n",
    "<img src=\"pics/tree1.png\" width=\"400px\">\n",
    "\n",
    "- From the above tree it is clear that when a new word come it will communicate to the other nodes in the tree which will have different values(based on the words of the sentence) and it will adjust it's value accordingly where in the result is obtained if the sentence is positive or negative when the leaf node is reached.\n",
    "\n",
    "- Result - So the the random forest will give us the result(if the sentence is positive or negative). Eg: Say if we have a word 80% negtive, it will communicate to the other nodes in the tree which will have different values and it will adjust it's value accordingly where in the result is obtained if the sentence is positive or negative when the leaf node is reached.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### II. <B>Main data study, data cleaning, EDA, Model application and Testing on a website  </B>\n",
    "\n",
    "For my project(Sentiment Analysis), I took the data set from the below link. The dataset is from \"Sentiment140\", a dataset originated from Stanford University. The dataset was of tweets which is labelled as either positive or negative.\n",
    "http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
    "\n",
    "\n",
    "<b>1. Data preparation :</b>\n",
    "\n",
    "The above link had tweets in english which was perfct for sentimnt analysis. The dataset I downloaded had 1600000 rows and 6 columns.The columns in the data were 'Label','id','date','query_string','user','Sentences'. \n",
    "\n",
    "\n",
    "<img src=\"pics/data.png\" width=\"700px\">\n",
    "\n",
    "I dropped the 'id', 'date', 'query_string', 'user' as I only wanted 'sentences' and 'lebels' columns for my model.\n",
    "\n",
    "\n",
    "Now when I looked indepth to the labeled columns :\n",
    "All postive sentences were label 4 and all negative sentences were label as 0, I converted the labeles to 0 for all negative and 1 for positive. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<b>2. Data cleaning :</b>\n",
    "\n",
    "\n",
    "\n",
    "- Data cleaning and preparing the data :\n",
    "\n",
    "    - Shape of the data - The data had 1600000 rows and 2 columns('sentences' and 'Lebels').\n",
    "    - Number of positive and negative sentences - The data had equal number of positive and negative sentences of 800000 each.\n",
    "    - Null values - My data did had 3373 null values.\n",
    "    - Shape of the data after removing null values - 1596627 rows and 2 columns\n",
    "    - Number of positive and Negative sentences - The data still had equal number of positive and negative sentences, 798467 positive and 798160 negative sentences.\n",
    "\n",
    "- Indepth Data cleaning:\n",
    "\n",
    "    - I made sure I removed all HTML Decoding, @mention, URL Links, UTF-8 BOM (Byte Order Mark), hashtag / numbers and Negations from my data. After removing all the above I had a dataset with clean sentences.\n",
    "The data looked as below:\n",
    "\n",
    "<img src=\"pics/pic_clesnsentences.png\" width=\"800px\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3. Exploratory Data Analysis : </b>\n",
    "\n",
    "As I had both positive and negative sentences I wanted to check what was the most occuring word except for all the stopwords(They are insignificat words Eg: the, a, an ... etc). So for this I used a word colud and I also prepared a chart for the same where I got an over all idea about the word that frequently occured in positive and Negative sentences.\n",
    "\n",
    "- EDA Plots for positive sentences\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"pics/graph_possen.png\" width=\"400px\"></td>\n",
    "<td><img src=\"pics/wordcloud_possent.png\" width=\"400px\"></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "- EDA Plots for Negative sentences\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"pics/graph_negsen.png\" width=\"400px\"></td>\n",
    "<td><img src=\"pics/worcloud_negsen.png\" width=\"400px\"></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "- You can see \"love\" word in both positive and negative sentences well it was because of sentences like the below\n",
    "    - I love this dress\n",
    "    - I do not love makeup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>4. Models Applied as a prilim study</b>\n",
    "\n",
    "- I tried applying various models like Logistic regression, Random forest, Gradient boost and Naive Bayes just to \n",
    " see how my data would respond to various models and what would be the time taken to run. My inference is shown below:\n",
    "    \n",
    "\n",
    "<img src=\"pics/models_result.png\" width=\"600px\">\n",
    "\n",
    "- As you can see form the above figure Random forest took the longet time to run when compared to other models. Well As random forest is considered as a collection of trees and it learns when it make each tree it was understood that it will take longer time. \n",
    "- The intresting fact here was that Logistic regression took only seconds to run 1600000 data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b>5. NEURAL NETWORK</b>\n",
    "\n",
    "- Now when I thought about any other model that had a better learning performance and Neural Network was the model that striked me.\n",
    "\n",
    "- What is Neural Network?\n",
    "    - Neural networks(NN) are typically organized in layers and each layer have vearious neurons with activation functions to reduce the loss. The activation functions that I have used here is 'Relu' and 'Sigmoid'. Once you send in your features to the neural network will communicate to other layers with randomized weights. When the neurons in the first layer communicate to the neurons in next layer with randomized weights we get the output, the error is also obtained and the Gradient descent for the error is calculated where the gradient descent would minimise the loss and then a back propogation is done to readjust the weights so as to have a better accuracy and minimum loss.\n",
    "\n",
    "    - A neural network can have an input layer , hidden layers(if the no. of hidden layers increase it becomes deep neural network) and an output layer\n",
    "\n",
    "<img src=\"pics/sigmoid_relu.png\" width=\"800px\">\n",
    "\n",
    "    - The activation function Relu keeps all the value < 0 as 0 and all the value > 0 as the original value.\n",
    "    - The activation function Sigmoid will have the values between 0 and 1.\n",
    "\n",
    "\n",
    "- So before moving into the the neural network to vectorise my sentences I had to use TFIDF\n",
    "\n",
    "- What is TFIDF?\n",
    "\n",
    "    - It weights the count for words appropriately.\n",
    "\n",
    "    - TF-IDF score of a word : TF(word) * IDF(word)\n",
    "\n",
    "    - TF(word) = No. of times the word appeared in the doc / tot. no. of words in the doc.\n",
    "\n",
    "    - IDF(word) = log(No. of docs/ No. of docs that contain word w)\n",
    "\n",
    "- In the below image you can see that the most frequently occuring words are given least value and the rarely occuring words are given high value\n",
    "\n",
    "<img src=\"pics/TFIDF_pic.png\" width=\"300px\">\n",
    "\n",
    "- For the TFIDF vectorization I also had to look into the n-grams and the max-features and the result was as below.\n",
    " The row marked in red fontcolor was the best and I used n(1,3)- where in the TFIDF will consider the uni-grams, bi-grams and tri-grams, and max-features - 100000(This means for 1600000 sentences it would keep 100000 words for the prediction of sentiment of a sentence)\n",
    "\n",
    "<img src=\"pics/n_grams.png\" width=\"600px\">\n",
    "\n",
    "\n",
    "- The network I used for Neural network and the neural netwok functioning is as mentioned below:\n",
    "\n",
    "<img src=\"pics/nn1-pic.png\" width=\"500px\">\n",
    "\n",
    "   - Load the data : Is the loading the 1600000 data.\n",
    "    - Clean the sentences : As mentioned above I am doing a deep cleaning on the data set by passing all the data through a function where all the data cleaning code statements is written.\n",
    "\n",
    "    - Bag of Words : It is a numerical representation of a piece of text. I am using TFIDF here to create my Bag of Words.\n",
    "\n",
    "    - Creating feature vectors : Once I create the bag of words with the help of TFIDF vectorizer I am vectorizing it and then converting it to array.\n",
    "\n",
    "    - Creating classifier - The classifier that I am using here is neural network.In the above network I am having input layer with 200 neurons and moving on I am having 2 dropout layers which will help in randomly switching on and off the neurons communicating to other layers to avoid overfitting. Including the dropout layer I have 5 hidden layers and 1 output layer.In the model.compile layer the optimiser used is adam(the best version of gradient descent to reduce the loss)\n",
    "\n",
    "- With this network I was able to achive <b>\" 84% accuracy on my Training data and 82% accuracy on my holdout or test data\"</b>.\n",
    "\n",
    "\n",
    "- Result - After going through the neural network, a prediction value is obtained where if the value > 0.5 the sentence is positive and if the value < 0.5 the sentence is negative.\n",
    "\n",
    "\n",
    "- The results of the neural netwok is as shown below:\n",
    "</br></br>\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"pics/roc_curve.png\" width=\"400px\"></td>\n",
    "<td><img src=\"pics/result_classification_metrix.png\" width=\"600px\"></td>\n",
    "</tr>\n",
    "<tr><img src=\"pics/loss_fun.png\" width=\"400px\">\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>6. WEBSITE</b>\n",
    "\n",
    "- After achiving a very good result on the neural network I wanted to test this on a website. Basically creating \n",
    " an interactive enviroment for the user so that they could use it for fun.\n",
    "\n",
    "- To achieve the above I used FLASK.\n",
    "\n",
    "- Flask is a micro web framework written in Python.Flask provides you with tools, libraries and technologies that allow you to build a web application. \n",
    " This web application can be some web pages, a blog, a wiki or go as big as a web-based calendar application or a commercial website.\n",
    "\n",
    "- My website looked as below:\n",
    "    \n",
    "<img src=\"pics/website.png\" width=\"800px\">\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>7. SAMPLE RESULTS FROM THE WEBSITE :</b>\n",
    "\n",
    "- NOTE : Prediction value  <  0.5  is  “Negative” And Prediction value  >  0.5  is “Positive”\n",
    "\n",
    "\n",
    "- I ain’t got no money\n",
    "\n",
    "    - Negative \n",
    "    - (0.09550709)\n",
    "\n",
    "- I love whatever i am doin\n",
    "\n",
    "    - positive \n",
    "    - (0.8527518)\n",
    "\n",
    "- That’s Awesome!!......\n",
    "\n",
    "    - positive \n",
    "    - (0.9294801)\n",
    "\n",
    "- Can't nobody beat 'em.\n",
    "\n",
    "    - Negative \n",
    "    - (0.19553101)\n",
    "\n",
    "- Won't anybody hit us.\n",
    "\n",
    "    - Negative \n",
    "    - (0.3599353)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kY4-HG9Hxh7c"
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "DonnaAlex_capstoneM2.ipynb",
   "provenance": [
    {
     "file_id": "1x5WzsOZU75qNBM2HaC-IdMaGOjAOJg9r",
     "timestamp": 1518650782969
    },
    {
     "file_id": "19pfSZyLw9OkkHDyvf3VYAnqg1U9GMJRA",
     "timestamp": 1518557285814
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
